# ai-analysis
Braindump about analyzing AI models for vulnerabilities/usefulness/etc.

The purpose of this document is to be a starting point for discussing how to evaluate the effectiveness and potential risks of AI large language models (LLMs). It is a work in progress and in no way is a "final word" on the subject. Critique and input are always welcomed.

If you don't know where to start, [start here](introduction.md).

## Other sources of info
- JailbreakBench ([github](https://github.com/JailbreakBench/jailbreakbench), [Hugging Face](https://huggingface.co/JailbreakBench)): An open-source robustness benchmark for jailbreaking large language models (LLMs).
- [Jailbreaking Black Box Large Language Models in Twenty Queries (github)](https://github.com/patrickrchao/JailbreakingLLMs): Code for the _Prompt Automatic Iterative Refinement_ (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. ([paper](https://arxiv.org/pdf/2310.08419))
- [jailbreakchat.com](https://jailbreakchat.com)
